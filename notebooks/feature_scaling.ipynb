{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx/63n+73H6vVveSBr5yPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stepsbtw/Machine-Learning/blob/main/notebooks/feature_scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling\n",
        "Transformar features numéricas para ranges específicos ou para ter propriedades estatísticas específicas.\n",
        "\n",
        "1. **Melhora a convergência do modelo**: Alguns algoritmos, especialmente aqueles baseados em otimização por gradiente (regressão logística, support vector machine, redes neurais).\n",
        "\n",
        "2. **Features com pesos semelhantes**: Em muitos algoritmos como no k-NN, SVM, k-means clustering, as features com grandes ranges numéricas vão estar a frente nas métricas de distância, levando a resultados enviesados. Scaling assegura que todas as features contribuem igualmente ao modelo.\n",
        "\n",
        "3. **Aumenta Acurácia do Modelo**: Algoritmos como PCA, que dependem de distâncias e covariâncias, o feature scaling assegura que o modelo não dará mais importância para features com magnitudes maiores. Pode ajudar na interpretabilidade e na acurácia.\n",
        "\n",
        "### Técnicas:\n",
        "- Min-Max Scaling (Normalization)\n",
        "- Standardization (Z-score Normalization)\n",
        "- Robust Scaling\n",
        "- Log Transformation"
      ],
      "metadata": {
        "id": "9G1xLEEyvQxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boas Práticas do Feature Scaling\n",
        "\n",
        "1. Ajuste o scaler apenas nos dados de **TREINO** -> Evita **data leakage**\n",
        "2. Escolher o scaler certo baseado na distribuição dos dados -> Assegura as transformações apropriadas.\n",
        "3. Aplique o mesmo scaler no treino e no teste -> Mantém consistência.\n",
        "4. Sempre cheque o efeito do scaling com visualizações -> Detecta problemas com dados transformados.\n",
        "5. Saiba quando o scaling é **desnecessário** -> Modelos baseados em árvore\n",
        "6. Salve e reutilize o scaler treinado em produção -> Consistência."
      ],
      "metadata": {
        "id": "2kqJ5eK4wm1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
        "from numpy import log1p"
      ],
      "metadata": {
        "id": "NBZrPtAFxdl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ajuste o Scaler somente em treino\n",
        "Previne data-leakage"
      ],
      "metadata": {
        "id": "P-fZZ_B61LQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# treine apenas com treino\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# transforme com o mesmo scaler\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "fB8PHVE71Rnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplique o mesmo Scaler em treino e teste\n"
      ],
      "metadata": {
        "id": "Zou_SXlO06V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# features numéricas e categóricas\n",
        "num_features = ['age', 'income', 'height']\n",
        "cat_features = ['gender', 'city']\n",
        "\n",
        "# criar o pipeline para mixed scaling\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), num_features),\n",
        "    (\"cat\", OneHotEncoder(), cat_features)\n",
        "])\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)  # mesma transformacao"
      ],
      "metadata": {
        "id": "lg3mZbLz1AE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nem sempre necessário\n",
        "Modelos em árvores : Decision Trees, Random Forests, XGBoost.\n",
        "Modelos baseados em regras: RuleFit, alguns ensembles.\n",
        "\n",
        "Mas é **importantíssimo** para:\n",
        "- Regressão Logística, SVM, k-NN, Neural Networks, PCA, Gradient Descent-based no geral."
      ],
      "metadata": {
        "id": "sAyZM08Z0RNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salve para produção"
      ],
      "metadata": {
        "id": "fsxUjlEL0yWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the fitted scaler\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# Load and use it in production\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "new_data_scaled = scaler.transform(new_data)"
      ],
      "metadata": {
        "id": "TZAgis3C0vfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sumário\n",
        "\n",
        "| **Método**      | **Fórmula**                     | **Efeito**                      | **Melhor caso de uso**                     | **Sensibilidade a Outliers** |\n",
        "|-------------------------|--------------------------------|--------------------------------|--------------------------------------|-----------------------------|\n",
        "| **Min-Max Scaling**     | $$x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$$ | Valores entre **0 e 1** | Features com **range fixa** (ex: Valor de pixel) | **ALTO** (Afetado por valores extremos) |\n",
        "| **Standardization**     | $$x' = \\frac{x - \\mu}{\\sigma}$$ | Centraliza em torno de **0** com desvio padrão **1** | Dados que seguem a **Distribuição Normal** | **Moderado** (Menos que Min-Max) |\n",
        "| **Robust Scaling**      | $$x' = \\frac{x - \\text{median}}{\\text{IQR}}$$ | Usa **média & desvio interquartílico**, robusto à outliers | Dados com **vários outliers** | **Baixo** (cuida bem de outliers) |\n",
        "| **Log Transformation**  | $$x' = \\log(1 + x)$$ | Reduz **assimetria à direita** | Dados com **caudas longas** (ex: Renda) | **Low** (Bom para dados assimétricos) |\n"
      ],
      "metadata": {
        "id": "BuJrNssZ1foW"
      }
    }
  ]
}